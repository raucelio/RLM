[["index.html", "Regressão Linear Múltipla Chapter 1 Introdução", " Regressão Linear Múltipla Chapter 1 Introdução "],["intro.html", "Chapter 2 Introdução a matrizes 2.1 Definições 2.2 Operações com matrizes 2.3 Propriedades de matrizes", " Chapter 2 Introdução a matrizes O estudo de Regressão Linear Múltipla requer conhecimento de notação matricial. Seguem as definições necessárias para o início do estudo desta técnica. Neste texto são apresentadas definições e operações utilizadas na regressão linear múltipla, já a operação com matrizes no R será apresentada em outro capítulo. 2.1 Definições Matriz é um arranjo retangular de elementos organizados em linhas e em colunas. Uma matriz é denotada por uma letra maíuscula e os elementos por letras minúsculas, como em A: \\[\\begin{equation*} A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; &amp; &amp; \\cdots &amp; \\\\ a_{m1} &amp; a_{m2} &amp; a_{m3} &amp; \\cdots &amp; a_{mn} \\\\ \\end{bmatrix} \\end{equation*}\\] Uma matriz com \\(m \\times n\\) elementos, ordenados em \\(m\\) linhas e \\(n\\) colunas, é uma matriz de ordem \\(m\\) por \\(n\\) e denotada por \\(m \\times n\\). Na notação \\(a_{ij}\\) o índice \\(i\\) indica a linha e o \\(j\\) a coluna. Exemplo \\[\\begin{equation*} A = \\begin{bmatrix} 2 &amp; 5 &amp; 10 \\\\ 3 &amp; 6 &amp; 12 \\end{bmatrix} \\end{equation*}\\] Matriz Quadrada é caracterizada por ter o número de linhas igual ao numero de colunas, \\(m = n\\). Exemplo \\[\\begin{equation*} A = \\begin{bmatrix} 8 &amp; 2 \\\\ 4 &amp; 10 \\end{bmatrix} \\end{equation*}\\] Em uma matriz quadrada, os elementos \\(a_{ij}\\), com \\(i = j\\), forma a diagonal principal. Matriz diagonal é caracterizada por ter os elementos \\(a_{ij} = 0\\) para todo \\(i \\neq j\\). Exemplo \\[\\begin{equation*} A = \\begin{bmatrix} 8 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 5 \\end{bmatrix} \\end{equation*}\\] Matriz identidade é uma matriz diagonal, cujos elementos da diagonal principal são iguais a 1. É donotada por I. Exemplo \\[\\begin{equation*} I = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\end{equation*}\\] Observe que \\(a_{ij} =0\\) para \\(i \\neq j\\) e \\(a_{ij} = 1\\) para \\(i = j\\). Matriz nula é uma matriz cujos elementos são iguais a 0. Exemplo \\[\\begin{equation*} I = \\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\end{equation*}\\] Observe que \\(a_{ij} =0\\) para \\(i \\neq j\\) e \\(a_{ij} = 1\\) para \\(i = j\\). Matriz transposta é o resultado da troca das linhas pelas colunas e denotada por \\(A^T\\) ou \\(A^\\prime\\). Exemplo Para \\[\\begin{equation*} A = \\begin{bmatrix} 2 &amp; 7 \\\\ 0 &amp; 1 \\\\ 2 &amp; 0 \\end{bmatrix} \\end{equation*}\\] a sua transposta \\(A^\\prime\\) é \\[\\begin{equation*} A^\\prime = \\begin{bmatrix} 2 &amp; 0 &amp; 2\\\\ 7 &amp; 1 &amp; 0 \\end{bmatrix} \\end{equation*}\\] Matriz Simétrica é uma matriz quadrada com a propriedade de ser igual a sua transposta. Assim, \\(A = A^\\prime\\). Exemplo \\[\\begin{equation*} A = \\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 2 &amp; 1 &amp; 4\\\\ 3 &amp; 4 &amp; 1 \\end{bmatrix} \\end{equation*}\\] Traço de matriz quadrada é a soma dos elementos da diagonal principal: \\[\\begin{equation*} Traço(A)= \\sum_{i,j} a_{ij} \\quad \\forall i = j \\end{equation*}\\] Exemplo Para \\[\\begin{equation*} A = \\begin{bmatrix} 7 &amp; 2 &amp; 3\\\\ 2 &amp; 8 &amp; 4\\\\ 3 &amp; 4 &amp; 9 \\end{bmatrix} \\end{equation*}\\] os elementos da diagonal principal são 7, 8 e 9, logo: \\[\\begin{equation*} Traço(A)= 7 + 8 + 9 = 24 \\end{equation*}\\] Determinante de matriz é um valor associado a uma matriz quadrada, A, e denotada por \\(|A|\\). Matriz singular é a matriz A cujo determinante é nulo, \\(|A| = 0\\). 2.2 Operações com matrizes Soma e subtração de duas matrizes com as mesmas dimensões podem ser somadas ou subtraídas adicionandos pela soma ou subtração dos elementos correspondentes. Exemplo \\[\\begin{equation*} \\begin{bmatrix} 7 &amp; 2 &amp; 3\\\\ 2 &amp; 8 &amp; 4\\\\ 3 &amp; 4 &amp; 9 \\end{bmatrix} + \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6\\\\ 7 &amp; 6 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 10 &amp; 10 &amp; 10\\\\ 10 &amp; 10 &amp; 10\\\\ 10 &amp; 10 &amp; 10 \\end{bmatrix} \\end{equation*}\\] Produto por escalar de uma matriz A por um escalar c é obtida multiplicando cada elemento de A pelo valor de c. Exemplo \\[\\begin{equation*} 5 \\times \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6\\\\ 7 &amp; 6 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 15 &amp; 40 &amp; 35\\\\ 40 &amp; 10 &amp; 30\\\\ 35 &amp; 30 &amp; 5 \\end{bmatrix} \\end{equation*}\\] Produto de matrizes só pode ser realizada caso o número de colunas da matriz que pré-multiplica for igual ao número de linhas da matriz que pós-multiplica. A operação consiste na soma dos produtos de cada elementos da linha da matriz que pré-multiplica pelos respectivos elementos da coluna da matriz que pós-multiplica. O resultado será uma matriz com o número de linhas da que pré-multiplica e com número de colunas das que pós-multiplica. Exemplo \\[\\begin{equation*} \\begin{bmatrix} 7 &amp; 2 \\\\ 2 &amp; 8 \\\\ \\end{bmatrix} + \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6 \\end{bmatrix} = \\begin{bmatrix} 7 \\times 3 + 2 \\times 8 &amp; 7 \\times 8 + 2 \\times 2 &amp; 7 \\times 7 + 2 \\times 6\\\\ 2 \\times 3 + 8 \\times 8 &amp; 2 \\times 8 + 8 \\times 8 &amp; 2 \\times 7 + 8 \\times 6 \\end{bmatrix} = \\begin{bmatrix} 37 &amp; 60 &amp; 61\\\\ 70 &amp; 80 &amp; 62 \\end{bmatrix} \\end{equation*}\\] Matriz inversa de uma matriz \\(A\\) é representada por \\(A^{-1}\\) e aquela que \\[\\begin{equation*} AA^{-1}=A^{-1}A = I \\end{equation*}\\] onde \\(I\\) é a matriz identidade. Para uma matriz \\(A\\) ter inversa é necessário e suficiente que seja quadrada e não sigular, isto é, o determinante é diferente de zero. Exemplo Para \\[\\begin{equation*} A = \\begin{bmatrix} 2 &amp; 1 &amp; 1\\\\ 1 &amp; 3 &amp; 1\\\\ 0 &amp; 1 &amp; 2 \\end{bmatrix} \\end{equation*}\\] a sua matriz inversa é \\[\\begin{equation*} A^{-1} = \\dfrac{1}{9} \\begin{bmatrix} 5 &amp; -1 &amp; -2\\\\ -2 &amp; 4 &amp; -1\\\\ 1 &amp; -2 &amp; 5 \\end{bmatrix} \\end{equation*}\\] Verificando \\(A^{-1}A=I\\) \\[\\begin{equation*} \\begin{bmatrix} 2 &amp; 1 &amp; 1\\\\ 1 &amp; 3 &amp; 1\\\\ 0 &amp; 1 &amp; 2 \\end{bmatrix} \\dfrac{1}{9} \\begin{bmatrix} 5 &amp; -1 &amp; -2\\\\ -2 &amp; 4 &amp; -1\\\\ 1 &amp; -2 &amp; 5 \\end{bmatrix}= \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\end{equation*}\\] 2.3 Propriedades de matrizes Considerando que as operação de multiplicação, transposição e inversão das matrizes A, B e C. São válidas as seguintes propriedades. Multiplicação \\(ABC=A(BC)=A(BC)\\) \\(A(B+C) = AB + AC\\) \\((B+C)A = BA + CA\\) Transposta de Matrizes \\(\\left(A^\\prime\\right)^\\prime = A\\) \\(\\left(A+B\\right)^\\prime= A^\\prime + B^\\prime\\) \\(\\left(AB\\right)^\\prime = B^\\prime A^\\prime\\) Inversão de Matrizes \\(\\left(A^{-1}\\right)^{-1} = A\\) \\(\\left(AB\\right)^{-1}= B^{-1}a^{-1}\\) \\(\\left(A^\\prime\\right)^{-1} = \\left(A^{-1}\\right)^\\prime\\) "],["cap3.html", "Chapter 3 Matrizes no R 3.1 Gerar matrizes no R 3.2 Transposta de uma matriz 3.3 Traço de uma matriz 3.4 Determinante de uma matriz 3.5 Matriz inversa 3.6 Operações com matrizes no R", " Chapter 3 Matrizes no R O R tem uma estrutura de dados que organiza os seus valores em linhas e colunas, cujos os elemetentos devem ser do mesmo tipo. Essa estrutura é uma matriz e quando os seus valores são numéricos o seu coportamento é igual ao de uma matriz. Uma matriz é um arranjo retangular de elementos organizados em linhas e em colunas. Uma matriz é denotada por uma letra maíuscula e os elementos por letras minúsculas, como em A: \\[\\begin{equation*} A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; &amp; &amp; \\cdots &amp; \\\\ a_{m1} &amp; a_{m2} &amp; a_{m3} &amp; \\cdots &amp; a_{mn} \\\\ \\end{bmatrix} \\end{equation*}\\] Uma matriz com \\(m \\times n\\) elementos, ordenados em \\(m\\) linhas e \\(n\\) colunas, é uma matriz de ordem \\(m\\) por \\(n\\) e denotada por \\(m \\times n\\). Na notação \\(a_{ij}\\) o índice \\(i\\) indica a linha e o \\(j\\) a coluna. 3.1 Gerar matrizes no R A função matrix() gera uma matriz com \\(m \\times n\\) valores organizados em \\(m\\) linhas e \\(n\\) colunas. A matriz \\[\\begin{equation*} A = \\begin{bmatrix} 2 &amp; 5 &amp; 10 \\\\ 3 &amp; 6 &amp; 12 \\end{bmatrix} \\end{equation*}\\] pode ser gerada pelo código abaixo. dados &lt;- c(2,3,5,6,10,12) m = 2 # Número de linhas n = 3 # Número de colunas A &lt;- matrix(dados, nrow = m, ncol = n) A ## [,1] [,2] [,3] ## [1,] 2 5 10 ## [2,] 3 6 12 Observe que a matriz gerada tem \\(m=2\\) linhas e \\(n=3\\) colunas e os \\(n\\times m = 6\\) valores são alocados por colunas, iniciando pela 1ª coluna da 1ª linha, esse comporpamento é o padrão e pode ser alterado pelo argumento byrow =T. Uma matriz quadrada é caracterizada por ter o número de linhas igual ao numero de colunas, \\(m = n\\). Assim, \\[\\begin{equation*} A = \\begin{bmatrix} 8 &amp; 2 \\\\ 4 &amp; 10 \\end{bmatrix} \\end{equation*}\\] A matriz acima é gerada por dados &lt;- c(8,4,2,10) m = 2 # Número de linhas n = 2 # Número de colunas A &lt;- matrix(dados, nrow = m, ncol = n) A ## [,1] [,2] ## [1,] 8 2 ## [2,] 4 10 Em uma matriz quadrada, os elementos \\(a_{ij}\\), com \\(i = j\\), forma a diagonal principal. Os elementos de uma diagonal podem ser obtidos pela função diag(). A &lt;- matrix(c(2,5,5,3),2,2) diag(A) ## [1] 2 3 Uma matriz diagonal é caracterizada por ter os elementos \\(a_{ij} = 0\\) para todo \\(i \\neq j\\). A função diag(), além de obter os elementos da diagonal principal, gera uma matriz diagonal. A matriz \\[\\begin{equation*} A = \\begin{bmatrix} 8 &amp; 0 &amp; 0\\\\ 0 &amp; 2 &amp; 0\\\\ 0 &amp; 0 &amp; 5 \\end{bmatrix} \\end{equation*}\\] pode ser criada pelo código aux &lt;- c(8,2,5) A&lt;- diag(aux) A ## [,1] [,2] [,3] ## [1,] 8 0 0 ## [2,] 0 2 0 ## [3,] 0 0 5 A Matriz identidade pode ser uma matriz diagonal, cujos elementos da diagonal principal são iguais a 1. A matriz \\[\\begin{equation*} I = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\end{equation*}\\] pode ser gerada por aux &lt;- rep(1,3) I &lt;- diag(aux) I ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Observe que \\(a_{ij} =0\\) para \\(i \\neq j\\) e \\(a_{ij} = 1\\) para \\(i = j\\). A Matriz nula tem todos os elementos iguais a 0. Por exemplo \\[\\begin{equation*} B = \\begin{bmatrix} 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} \\end{equation*}\\] é gerada pela função matrix() com o valor 0 (zero) sendo o único para preencher todas as possições da matriz. B &lt;- matrix (0, nrow=2, ncol=3) B ## [,1] [,2] [,3] ## [1,] 0 0 0 ## [2,] 0 0 0 3.2 Transposta de uma matriz Matriz transposta é o resultado da troca das linhas pelas colunas e denotada por \\(A^T\\) ou \\(A^\\prime\\). Para \\[\\begin{equation*} A = \\begin{bmatrix} 2 &amp; 7 \\\\ 0 &amp; 1 \\\\ 2 &amp; 0 \\end{bmatrix} \\end{equation*}\\] a sua transposta \\(A^\\prime\\) é \\[\\begin{equation*} A^\\prime = \\begin{bmatrix} 2 &amp; 0 &amp; 2\\\\ 7 &amp; 1 &amp; 0 \\end{bmatrix} \\end{equation*}\\] A função t() gera a matriz transposta de uma matriz. Observe o códio seguinte aux &lt;- c(2,0,2,7,1,0) A &lt;- matrix (aux,nrow=3,ncol=2) tranposta_A &lt;- t(A) A ## [,1] [,2] ## [1,] 2 7 ## [2,] 0 1 ## [3,] 2 0 tranposta_A ## [,1] [,2] [,3] ## [1,] 2 0 2 ## [2,] 7 1 0 3.3 Traço de uma matriz Traço de matriz quadrada é a soma dos elementos da diagonal principal de uma matriz: \\[\\begin{equation*} Traço(A)= \\sum_{i,j} a_{ij} \\quad \\forall i = j \\end{equation*}\\] Para \\[\\begin{equation*} A = \\begin{bmatrix} 7 &amp; 2 &amp; 3\\\\ 2 &amp; 8 &amp; 4\\\\ 3 &amp; 4 &amp; 9 \\end{bmatrix} \\end{equation*}\\] os elementos da diagonal principal são 7, 8 e 9, logo: \\[\\begin{equation*} Traço(A)= 7 + 8 + 9 = 24 \\end{equation*}\\] O traço de A pode ser obtido por meio das funções diag() e sum() como abaixo. aux&lt;-c(7,2,3,2,8,4,3,4,9) A&lt;- matrix(aux,3,3) sum(diag(A)) ## [1] 24 3.4 Determinante de uma matriz O Determinante de matriz é um valor associado a uma matriz quadrada, A, e denotada por \\(|A|\\). A função det() calcula o determinante de uma matriz. aux&lt;-c(7,2,3,2,8,4,3,4,9) A&lt;- matrix(aux,3,3) det(A) ## [1] 332 A Matriz singular é a matriz A cujo determinante é nulo, \\(|A| = 0\\). A matriz B é singular aux&lt;-c(7,2,3,14,4,6,3,4,9) A&lt;- matrix(aux,3,3) det(A) ## [1] 0 Propriedades do determinante: O determinante de uma matriz quadrada A é igual ao determinante da sua transposta: \\(|A| = |A^\\prime|\\); Caso exista uma linha ou coluna na matriz igual a zero, o determinante é zero; Caso exista duas filas paralelas, iguais ou proporcional, o determinante é zero; O determinante do produto de um número real k por uma matriz A é igual ao produto de k elevado a n, onde n é o número de linhas de A, pelo determinante de A: \\(|k . A| = k^n . |A|\\); Caso os elementos abaixo ou acima da diagonal principal forem nulos, o determinante será o produto dos elementos da diagonal principal; Teorema de Binet: Seja A e B matrizes quadradas de ordem n, o determinante do produto de A por B é igual ao produto dos determinantes de A e B: \\(|AB|=|A|.|B|\\). 3.5 Matriz inversa A Matriz inversa de uma matriz \\(A\\) é representada por \\(A^{-1}\\) é aquela que \\[\\begin{equation*} AA^{-1}=A^{-1}A = I \\end{equation*}\\] onde \\(I\\) é a matriz identidade. Para uma matriz \\(A\\) ter inversa é necessário e suficiente que seja quadrada e não sigular, isto é, o determinante é diferente de zero. Para \\[\\begin{equation*} A = \\begin{bmatrix} 2 &amp; -5\\\\ -1 &amp; 3 \\end{bmatrix} \\end{equation*}\\] a sua matriz inversa é \\[\\begin{equation*} A^{-1} = \\begin{bmatrix} 3 &amp; 5\\\\ 1 &amp; 2 \\end{bmatrix} \\end{equation*}\\] Verificando \\(A^{-1}A=I\\) \\[\\begin{equation*} \\begin{bmatrix} 2 &amp; -5\\\\ -1 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 3 &amp; 5\\\\ 1 &amp; 2 \\end{bmatrix}= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\end{equation*}\\] A função solve() é usada para resulver sistemas de equações lineares e em uma de suas chamadas retornar a matriz inversa do seu argumento. A = matrix(c(2,-1,-5,3),2,2) inv_A = solve(A) inv_A ## [,1] [,2] ## [1,] 3 5 ## [2,] 1 2 para verificar a relaçao entre a sua matriz e a suam matriz inversa A %*% inv_A ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 3.6 Operações com matrizes no R A Soma ou subtração de duas matrizes com as mesmas dimensões é obtida pela soma/subtração dos elementos correspondentes. O operador para essas operações são + e -. \\[\\begin{equation*} \\begin{bmatrix} 7 &amp; 2 &amp; 3\\\\ 2 &amp; 8 &amp; 4\\\\ 3 &amp; 4 &amp; 9 \\end{bmatrix} + \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6\\\\ 7 &amp; 6 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 10 &amp; 10 &amp; 10\\\\ 10 &amp; 10 &amp; 10\\\\ 10 &amp; 10 &amp; 10 \\end{bmatrix} \\end{equation*}\\] A &lt;- matrix (c(7,2,3,2,8,4,3,4,9),3,3) B &lt;- matrix (c(3,8,7,8,2,6,7,6,1),3,3) A ## [,1] [,2] [,3] ## [1,] 7 2 3 ## [2,] 2 8 4 ## [3,] 3 4 9 B ## [,1] [,2] [,3] ## [1,] 3 8 7 ## [2,] 8 2 6 ## [3,] 7 6 1 A+B ## [,1] [,2] [,3] ## [1,] 10 10 10 ## [2,] 10 10 10 ## [3,] 10 10 10 O Produto por escalar de uma matriz A por um escalar c é obtida multiplicando cada elemento de A pelo valor de \\(c \\in \\mathbf{R}\\). O operador para essa operação *. \\[\\begin{equation*} 5 \\times \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6\\\\ 7 &amp; 6 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 15 &amp; 40 &amp; 35\\\\ 40 &amp; 10 &amp; 30\\\\ 35 &amp; 30 &amp; 5 \\end{bmatrix} \\end{equation*}\\] A &lt;- matrix (c(3,8,7,8,2,6,7,6,1),3,3) A ## [,1] [,2] [,3] ## [1,] 3 8 7 ## [2,] 8 2 6 ## [3,] 7 6 1 5*A ## [,1] [,2] [,3] ## [1,] 15 40 35 ## [2,] 40 10 30 ## [3,] 35 30 5 Na situação quando duas matrizes A e B tem o mesmo número de linhas e colunas o operador * executa o produto elemento a elemento. \\[\\begin{equation*} \\begin{bmatrix} 7 &amp; 2 &amp; 3\\\\ 2 &amp; 8 &amp; 4\\\\ 3 &amp; 4 &amp; 9 \\end{bmatrix} \\circ \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6\\\\ 7 &amp; 6 &amp; 1 \\end{bmatrix} = \\begin{bmatrix} 21 &amp; 16 &amp; 21\\\\ 16 &amp; 64 &amp; 24\\\\ 21 &amp; 24 &amp; 9 \\end{bmatrix} \\end{equation*}\\] No R seria A &lt;- matrix(c(7,2,3,2,8,2,3,4,8),3,3) B &lt;- matrix(c(3,8,7,8,2,6,7,6,1),3,3) A ## [,1] [,2] [,3] ## [1,] 7 2 3 ## [2,] 2 8 4 ## [3,] 3 2 8 B ## [,1] [,2] [,3] ## [1,] 3 8 7 ## [2,] 8 2 6 ## [3,] 7 6 1 A*B ## [,1] [,2] [,3] ## [1,] 21 16 21 ## [2,] 16 16 24 ## [3,] 21 12 8 O Produto de matrizes só pode ser realizada caso o número de colunas da matriz que pré-multiplica for igual ao número de linhas da matriz que pós-multiplica. A operação consiste na soma dos produtos de cada elementos da linha da matriz que pré-multiplica pelos respectivos elementos da coluna da matriz que pós-multiplica. O resultado será uma matriz com o número de linhas da que pré-multiplica e com número de colunas das que pós-multiplica. O operador para essas operações são %*%. \\[\\begin{equation*} \\begin{bmatrix} 7 &amp; 2 \\\\ 2 &amp; 8 \\\\ \\end{bmatrix} + \\begin{bmatrix} 3 &amp; 8 &amp; 7\\\\ 8 &amp; 2 &amp; 6 \\end{bmatrix} = \\begin{bmatrix} 37 &amp; 60 &amp; 61\\\\ 70 &amp; 80 &amp; 62 \\end{bmatrix} \\end{equation*}\\] A &lt;- matrix(c(7,2,2,8),2,2) B &lt;- matrix(c(3,8,8,2,7,6),nrow = 2, ncol =3) A ## [,1] [,2] ## [1,] 7 2 ## [2,] 2 8 B ## [,1] [,2] [,3] ## [1,] 3 8 7 ## [2,] 8 2 6 A%*%B ## [,1] [,2] [,3] ## [1,] 37 60 61 ## [2,] 70 32 62 "],["regressão-linear-múltipla.html", "Chapter 4 Regressão Linear Múltipla 4.1 Modelo de Regressão Linear Múltipla 4.2 Estimadores dos parâmetros", " Chapter 4 Regressão Linear Múltipla 4.1 Modelo de Regressão Linear Múltipla Seja a relação linear entre uma variável dependente Y e p variáveis independentes X. Então, o modelo estatístico de uma regressão linear, nos parâmetros, múltipla com p variáveis independentes e um termo aleatório, \\(\\epsilon\\), é dado por \\[\\begin{equation*} Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + + \\beta_pX_{pi} + \\epsilon_i \\end{equation*}\\] com \\(i=1,2,\\ldots,n\\). De forma alternativa, tem-se \\[\\begin{equation*} Y_i = \\beta_0 + \\sum_{j=1}^p\\beta_j X_{ji} + \\epsilon_i \\end{equation*}\\]. Uma outra forma de expressar as relações, fazendo \\(i=1,2,\\ldots, n\\), surgem as equações: \\[\\begin{align} Y_1 &amp;= \\beta_0 + \\beta_1 X_{11} + \\beta_2 X_{21} + \\cdots + \\beta_p X_{p1}\\\\ Y_2 &amp;= \\beta_0 + \\beta_1 X_{12} + \\beta_2 X_{22} + \\cdots + \\beta_p X_{p2}\\\\ Y_3 &amp;= \\beta_0 + \\beta_1 X_{13} + \\beta_2 X_{23} + \\cdots + \\beta_p X_{p3}\\\\ \\vdots &amp; \\\\ Y_n &amp;= \\beta_0 + \\beta_1 X_{1n} + \\beta_2 X_{2n} + \\cdots + \\beta_p X_{pn} \\end{align}\\] Em notação matricial, esse sistema de equações pode ser expressa por \\[\\begin{equation*} Y = X\\beta + \\epsilon \\end{equation*}\\] que de forma explícita é \\[\\begin{equation*} \\begin{bmatrix} Y_1\\\\ Y_2\\\\ \\vdots \\\\ Y_n \\end{bmatrix}= \\begin{bmatrix} 1 &amp; X_{11} &amp; X_{12} &amp; \\cdots &amp; X_{1p} \\\\ 1 &amp; X_{21} &amp; X_{22} &amp; \\cdots &amp; X_{2p} \\\\ \\vdots &amp; &amp; \\cdots &amp; \\\\ 1 &amp; X_{n1} &amp; X_{n2} &amp; \\cdots &amp; X_{np} \\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0\\\\ \\beta_1\\\\ \\beta_2\\\\ \\vdots \\\\ \\beta_p \\end{bmatrix}+ \\begin{bmatrix} \\epsilon_0\\\\ \\epsilon_1\\\\ \\epsilon_2\\\\ \\vdots \\\\ \\epsilon_p \\end{bmatrix} \\end{equation*}\\] Sendo n o número de observações e p a quantidade de variáveis explicativas; X uma matriz \\(n \\times (p+1)\\); Y um vetor \\(n \\times 1\\); \\(\\beta\\) um vetor \\((p+1) \\times 1\\); e \\(\\epsilon\\) um vetor \\(n \\times 1\\). O problema consiste em obter o modelo ajustado: \\[\\begin{equation*} \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_{1i} + \\hat{\\beta}_2X_{2i} + \\cdots + + \\hat{\\beta}_p X_{pi} \\end{equation*}\\] É, para isso, deve-se obter o vetor \\(\\beta\\). Admita-se as seguintes pressuposições: A variável dependente \\(Y\\) é a função linear das variáveis independentes X. Os valores das variáveis independentes são fixos. A média dos erros é nula, isto é, \\(E\\left(\\epsilon_i\\right)=0\\). Os erros são homoscedásticos, assim, \\(V\\left(\\epsilon_i\\right)=\\sigma^2\\). Os erros são não correlacionados entre si, isto é, \\(E\\left(\\epsilon_i\\epsilon_j\\right)=0\\), para \\(i \\ne j\\). Os erros têm distribuição normal. Considere algumas consequências: Combine (4) e (5) para \\(E\\left(\\epsilon^\\prime\\epsilon\\right)=I\\sigma^2\\). As pressuposições (1), (2) e (3) são necessárias para demostrar que os estimadores de Mínimos Quadrados são não tendenciosos. As pressuposições de (1) a (5) permitem demonstrar que tais estimadores são não tendenciosos e de variância mínima. A pressuposição (6) é necessária para construção de teste de hipóteses e de intervalos de confiânça para os parãmetros. 4.2 Estimadores dos parâmetros O Método dos Mínimos Quadrados consiste em adotar como estimativas dos parâmetros os valores que minimiza a soma de quadrados dos desvios. O modelo \\(Y = X\\beta+\\epsilon\\) têm \\(\\epsilon = Y - X\\beta\\), então a soma dos quadrados dos desvios é dada por \\[\\begin{align} Z &amp;= \\epsilon^\\prime\\epsilon \\\\ &amp;= \\left(Y-X\\beta\\right)^\\prime\\left(Y-X\\beta\\right)\\\\ &amp;= Y^\\prime Y - Y^\\prime X\\beta - \\beta^\\prime X^\\prime Y + \\beta^\\prime X^\\prime X \\beta \\end{align}\\] Como \\(Y^\\prime X\\beta = \\beta^\\prime X^\\prime Y\\) são iguais, então \\[\\begin{align} Z &amp;= Y^\\prime Y -2 \\beta^\\prime X^\\prime Y + \\beta^\\prime X^\\prime X \\beta \\end{align}\\] A função \\(Z\\) apresenta seu valor mínimo para \\(\\beta\\) que tornem a diferencial de \\(Z\\) e igualar a 0 (zero). \\[\\begin{equation*} \\dfrac{\\partial Z}{\\partial \\beta}= -2\\left(\\partial\\beta^\\prime\\right) X^\\prime Y + \\left(\\partial\\beta^\\prime\\right) X^\\prime X\\hat{\\beta} + \\hat{\\beta}^\\prime X^\\prime X\\left(\\partial \\beta\\right) = 0 \\end{equation*}\\] Uma vez que \\(\\left(\\partial\\beta^\\prime\\right)X^\\prime X\\hat{\\beta} =\\hat{\\beta}^\\prime X^\\prime X\\left(\\partial \\beta\\right)\\) tem-se \\[\\begin{align} \\dfrac{\\partial Z}{\\partial \\beta}&amp;= -2\\left(\\partial\\beta^\\prime\\right) X^\\prime Y + 2\\left(\\partial\\beta^\\prime\\right) X^\\prime X\\hat{\\beta} \\\\ &amp;=2\\left(\\partial\\beta^\\prime\\right)\\left[X^\\prime X\\hat{\\beta} -X^\\prime Y\\right] \\end{align}\\] Assim, para definir \\(\\hat{\\beta}\\) faça \\[\\begin{equation*} X^\\prime X\\hat{\\beta}-X^\\prime Y = 0 \\end{equation*}\\] Logo, \\[\\begin{equation*} \\hat{\\beta} = \\left(X^\\prime X\\right)^{-1}X^\\prime Y. \\end{equation*}\\] "],["ajuste-do-modelo.html", "Chapter 5 Ajuste do modelo", " Chapter 5 Ajuste do modelo "],["análise-de-variância.html", "Chapter 6 Análise de variância 6.1 Soma dos Quadrados dos Resíduos - SQRes 6.2 Soma dos Quadrados Total - SQTot 6.3 Soma do Quadrado de Regressão - SQReg 6.4 Tabela de análise de variância 6.5 Teste t para os parâmetros do modelo 6.6 Coeficiente de determinação", " Chapter 6 Análise de variância 6.1 Soma dos Quadrados dos Resíduos - SQRes Sendo que os valores previstos de \\(Y\\) é dado por \\[\\begin{equation*} \\hat{Y}=X\\hat{\\beta} \\end{equation*}\\] com \\[\\begin{equation*} \\hat{\\epsilon} = Y - \\hat{Y} \\end{equation*}\\] Assim, a Soma dos Quadrado dos Resíduos (SQRes) é dado por; \\[\\begin{align} SQRres &amp;= \\hat{\\epsilon}^ \\prime \\epsilon \\\\ &amp;= \\left(Y-\\hat{Y}\\right)^\\prime \\left(Y - \\hat{Y}\\right)\\\\ &amp;= \\left(Y-X\\hat{\\beta}\\right)^\\prime \\left(Y - X\\hat{\\beta}\\right)\\\\ &amp;=Y^\\prime -2\\hat{\\beta}^\\prime X^\\prime Y + \\hat{\\beta}^\\prime X^\\prime X \\hat{\\beta} \\end{align}\\] mas \\[\\begin{align} X^\\prime X \\hat{\\beta} &amp;= X^\\prime Y \\end{align}\\] logo \\[\\begin{align} SQRes &amp;= Y^\\prime Y - \\hat{\\beta}^\\prime X^\\prime Y \\end{align}\\] 6.2 Soma dos Quadrados Total - SQTot A SQtot pode ser obtida pela operação \\[\\begin{align} SQTot &amp;= \\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2 \\\\ &amp;= Y^\\prime \\left[I - \\dfrac{\\mu\\mu^\\prime}{n}\\right] \\\\ &amp;= Y^\\prime Y - \\left[\\dfrac{Y^\\prime \\mu\\mu Y}{n}\\right] \\end{align}\\] onde \\[\\begin{equation*} C=\\left[\\dfrac{Y^\\prime \\mu\\mu Y}{n}\\right] \\end{equation*}\\] é o fator de correção, logo \\[\\begin{equation*} SQtot = Y^\\prime Y - C \\end{equation*}\\] e o fator de correção pode ser obtido por \\[\\begin{equation*} C=\\dfrac{\\left(\\sum_{i=1}^2 Y_i\\right)^2}{n}. \\end{equation*}\\] 6.3 Soma do Quadrado de Regressão - SQReg A SQReg é dada por \\[\\begin{align} SQReg &amp;=\\sum_{i=1}^n\\left(\\hat{Y}_i - \\bar{Y}\\right)^2\\\\ &amp;= \\sum_{i=1}^n \\hat{Y}_i^2 - \\dfrac{\\left(\\sum_{i=1}^2 \\hat{Y}_i\\right)^2}{n} \\end{align}\\] Em forma matricial \\[\\begin{align} SQreg &amp;= \\hat{Y}^\\prime \\hat{Y } - \\dfrac{Y^\\prime \\mu \\mu^\\prime Y}{n} \\\\ &amp;= \\hat{Y}^\\prime \\hat{Y } - C, \\end{align}\\] onde \\[\\begin{equation*} C = \\dfrac{Y^\\prime \\mu \\mu^\\prime Y}{n} \\end{equation*}\\]. mas \\[\\begin{equation*} \\hat{Y} = X\\hat{\\beta} \\end{equation*}\\]. Assim, \\[\\begin{equation*} SQReg = \\hat{\\beta}^\\prime X^\\prime X\\hat{\\beta} - C \\end{equation*}\\], porém \\[\\begin{equation*} X^\\prime X \\hat{\\beta} = X^\\prime Y \\end{equation*}\\]. Logo \\[\\begin{equation*} SQReg = \\hat{\\beta}^\\prime X^\\prime Y - C \\end{equation*}\\] 6.4 Tabela de análise de variância FV GL SQ QM F regressão p \\(\\hat{\\beta}^\\prime X^\\prime Y - C\\) QMReg \\(\\dfrac{QMReg}{QMRes}\\) resíduo n-p-1 \\(Y^\\prime Y - \\hat{\\beta}^\\prime X^\\prime Y\\) QMRes total n-1 \\(Y^\\prime Y - C\\) É possível demonstrar que se os erros \\(\\epsilon\\) tiverem distribuição normal, a razão F possui distribuição F-snedecor com p e n-p-1 graus de liberdade se \\(\\beta_1=\\beta_2 = \\cdots = \\beta_p=0\\). Assim, F é a estatística teste para \\[ \\begin{cases} H_0:\\beta_1=\\beta_2 = \\cdots = \\beta_p=0 \\\\ H_1:\\exists \\quad \\beta_j \\neq 0 \\quad \\text { para } i = 1,\\ldots,p. \\end{cases} \\] 6.5 Teste t para os parâmetros do modelo As estimativas dos desvios-padrão do parâmetros \\(\\beta\\), são dadas pelas raízes quadradas dos elementod da diagonal principal da matriz \\((X^\\prime X)^{-1}\\hat{\\sigma}^2\\), onde \\(\\sigma^2\\) é estimado por \\[\\begin{equation*} \\hat{sigma}^2 = \\frac{QMRes}{n-p-1}= \\dfrac{Y^\\prime Y - \\hat{\\beta}^\\prime X^\\prime Y}{n-p-1} \\end{equation*}\\] onde \\(n\\) - número de observações. \\(p\\) - número de variáveis dependentes. \\(X\\) - matriz de dados. \\(Y\\) - dados observados. Pode-se provar que a estatística \\[\\begin{equation*} t=\\dfrac{\\hat{\\beta}_j - \\beta_j}{\\hat{\\sigma}(\\hat{\\beta}_j)} \\end{equation*}\\] tem distribuição t-Student com \\(n-p-1\\) graus de liberdade e \\(\\hat{\\sigma}(\\hat{\\beta}_j)\\) é o desvio-padrão do parametro \\(\\hat{\\beta}_j\\). Essa estatística t pode ser usada no teste de hipótese para cada parâmetro. \\[\\begin{cases} H_0: \\beta_j= 0 \\\\ H_1: \\beta_j \\neq 0 \\end{cases}\\] ou para construir o intervalo de confiança para \\(\\beta_j\\) com nível de confiança \\(\\gamma = 1-\\alpha\\), onde \\(0 &lt; \\alpha &lt; 1\\), por meio de \\[\\begin{equation*} \\hat{\\beta}_j - t_{\\alpha/2} \\hat{\\sigma(\\hat(\\beta)_j)} &lt; \\beta_j &lt; \\hat{\\beta}_j - t_{\\alpha/2} \\hat{\\sigma(\\hat(\\beta)_j)} \\end{equation*}\\] Para ser possível executar o teste e construir o intervalor de confiançã é necessário que os erros \\(\\epsilon_j\\) tenha distribuição normal. 6.6 Coeficiente de determinação O coeficiente de determinação é definida por \\[\\begin{equation*} R^2 = \\dfrac{SQReg}{SQTot}, \\end{equation*}\\] com \\(0 \\leq R^2 \\leq 1\\) que representa a proporção da variação total explicada pelo modelo de regressão linear múltipla. Uma propriedade importante do \\(R^2\\) que o seu valor aumenta com o aumento do número de variáveis independentes, assim para comparar dois modelos com número de variáveis independentes e é definido por \\[\\begin{equation*} \\bar{R}^2 = \\dfrac{R^2(n-1)-p}{n-p-1} \\end{equation*}\\] "],["análise-de-resíduos.html", "Chapter 7 Análise de Resíduos 7.1 Pontos de alavancagem 7.2 Resíduos Crus (ordinários) 7.3 Resíduos padronizados 7.4 Resíduos studentizados 7.5 Estatísticas baseadas em leave-one-out 7.6 Distância de Cook 7.7 DFfits 7.8 DFbetas", " Chapter 7 Análise de Resíduos 7.1 Pontos de alavancagem Os valores de alavancagem representam o peso que cada observação tem sobre o seu valor predito considerando o modelo construído. São os elementos da diagonal da matriz de projeção H. \\[\\begin{align*} h_{i}&amp;=H{ii} \\\\ h&amp;=diag(H)=diag(X(X^\\prime X)^{1}X^\\prime) \\end{align*}\\] A soma desses valores é p, ou seja, é o traço da matriz. Em média, o peso de cada observação é p/n. Atenção com as observações que possuem um valor de alavancagem duas vezes superior ao esperado. 7.2 Resíduos Crus (ordinários) Os resíduos são a diferença entre valores observados e ajustados. Estão na escala da própria resposta. O valor esperado é 0.\\(\\sigma^2(IH)\\). Têm valor esperado igual a zero. Embora seja uma suposição do modelo a independência condicional de y a x por meio do modelo, ao contrário do que se imagina, os resíduos crus não são independentes. É por essa razão que não se recomenda aplicar testes de hipótese, como de normalidade, aos resíduos crus, por exemplo. \\[\\begin{aligned} \\hat{e}_i &amp;= y_i - \\hat{y}_i\\\\ \\hat{e} &amp;= y - \\hat{y}\\\\ \\hat{e} &amp;= y - X\\hat{\\beta} \\end{aligned}\\] 7.3 Resíduos padronizados Os resíduos padronizados são resultado da padronização dos resíduos crus. Ao padronizar, ou seja, dividir pela correspondente variância \\(\\sigma^2(IH)\\), têm-se resíduos com variância unitária. Esses resíduos são chamados também de resíduos internamente studentizados. \\[\\begin{equation*} \\hat{r}_i = \\dfrac{\\hat{e}_i}{s(\\hat{e}_i)} = \\dfrac{\\hat{e}_i}{\\hat{\\sigma}\\sqrt{1-h_{i}}} \\end{equation*}\\] 7.4 Resíduos studentizados Os resíduos studentizados são considerados independentes pelo fato de serem resíduos decorrentes de procedimento leave-one-out. Para todos os efeitos, é como se o resíduo padronizado da observação i fosse calculado removendo-se o i-ésimo registro e ajustado o modelo. Não há necessidade de remover uma observação a cada vez para calcular tais resíduos pois tem-se fórmulas apropriadas para isso. Com isso, tem-se que yi e yi(i) são independentes. Testes de influência consideram esses resíduos. \\[\\begin{aligned} \\hat{t}_i &amp;= \\dfrac{\\hat{e}_i}{s(\\hat{e}_i)} = \\dfrac{\\hat{e}_i}{\\hat{\\sigma}_{-i}\\sqrt{1-h_{i}}} = \\hat{r}_i\\left(\\frac{n-p-1}{n-p-\\hat{r}_i^2} \\right)^{1/2}\\\\ \\hat{\\sigma}_{-i}^2 &amp;= \\dfrac{(n-p)\\hat{\\sigma}^2-\\frac{\\hat{e}_i^2}{1-h_{i}}}{(n-1)-p} \\end{aligned}\\] 7.5 Estatísticas baseadas em leave-one-out Os n vetores de estimativas dos parâmetros considerando a remoção da i-ésima observação em cada vez são obtidas por \\[\\begin{aligned} \\hat{beta}_{(-i)} = \\hat{\\beta}-\\hat{e}_i\\frac{(X^\\top X)^{-1} x_{i}}{1-h_{i}}. \\end{aligned}\\] A partir dessa medida, são obtidos os valores preditos, sem a i-ésima observação também, por \\[\\begin{aligned} \\hat{y}_{(-i)} = x_i^\\top\\hat{\\beta}_{(-i)}. \\end{aligned}\\] 7.6 Distância de Cook \\[\\begin{aligned} D_i &amp;= \\frac{(\\hat{\\beta}_{(-i)}-\\hat{\\beta})^\\top (X^\\top X) (\\hat{\\beta}_{(-i)}-\\hat{\\beta})}{p\\hat{\\sigma}^2}\\\\ &amp;= \\dfrac{(\\hat{y}-\\hat{y}_{i(-i)})^\\top (\\hat{y}-\\hat{y}_{i(-i)})}{p\\hat{\\sigma}^2} \\\\ &amp;= \\dfrac{1}{p}\\cdot\\dfrac{h_i}{(1-h_i)} \\cdot\\dfrac{\\hat{e}_i^2}{\\hat{\\sigma}^2(1-h_i)}\\\\ &amp;= \\dfrac{1}{p}\\cdot\\dfrac{h_i}{(1-h_i)}\\cdot r_i^2 \\end{aligned}\\] 7.7 DFfits \\[\\begin{equation*} dffits_i = \\dfrac{\\hat{y}_i-\\hat{y}_{i(-i))}}{\\hat{\\sigma}_{-i}\\sqrt{h_i}} = \\hat{t}_i\\left( \\dfrac{h_i}{1-h_i} \\right )^{1/2} \\end{equation*}\\] 7.8 DFbetas \\[\\begin{aligned} dbetas_i &amp;= \\dfrac{\\hat{\\beta}- \\hat{\\beta}_{(-i)}}{\\hat{\\sigma}_{(-i)} \\sqrt{\\text{diag}((X^\\top X)^{-1})}} \\end{aligned}\\] "],["exemplos.html", "Chapter 8 Exemplos 8.1 Exemplo 1 8.2 Exemplo 2 8.3 Exemplo 3", " Chapter 8 Exemplos 8.1 Exemplo 1 Em uma economia fictícia, o objetivo é analisar os índices da bolsa de valores (stock_index_price), a variável dependente, com base em duas variáveis independentes: Taxa de juro Taxa de desemprego. O dados analisados são os valores mensais observados no anos de 2016 e 2017. Ano Mês Taxa de juros Taxa de desemprego Preço da ação 2017 12 2.75 5.30 1464 2017 11 2.50 5.30 1394 2017 10 2.50 5.30 1357 2017 09 2.50 5.30 1293 2017 08 2.50 5.40 1256 2017 07 2.50 5.60 1254 2017 06 2.50 5.50 1234 2017 05 2.25 5.50 1195 2017 04 2.25 5.50 1159 2017 03 2.25 5.60 1167 2017 02 2.00 5.70 1130 2017 01 2.00 5.90 1075 2016 12 2.00 6.00 1047 2016 11 1.75 5.90 965 2016 10 1.75 5.80 943 2016 09 1.75 6.10 958 2016 08 1.75 6.20 971 2016 07 1.75 6.10 949 2016 06 1.75 6.10 884 2016 05 1.75 5.10 866 2016 04 1.75 5.90 876 2016 03 1.75 6.20 822 2016 02 1.75 6.20 704 2016 01 1.75 6.10 719 library(ggplot2) Ano &lt;- c (2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017,2017, 2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016,2016) Mes &lt;- c (12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1) taxa_de_juros &lt;- c (2.75,2.50,2.50,2.50,2.50,2.50,2.50,2.25,2.25,2.25,2.00,2.00, 2.00,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75, 1.75) taxa_de_desemprego &lt;- c (5.30,5.30,5.30,5.30,5.40,5.60,5.50,5.50,5.50,5.60,5.70,5.90, 6.00,5.90,5.80,6.10,6.20,6.10,6.10,6.10,5.90,6.20,6.20,6.10) stock_index_price &lt;- c (1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075, 1047,0965,0943,0958,0971,0949,0884,0866,0876,0822,0704,0719) dados &lt;- data.frame (stock_index_price, taxa_de_juros, taxa_de_desemprego) Visualização dos dados Em uma regressão linear é necessário que cada variável independente tenha relação linear com a variável dependente. Essa relação pode ser verificado por gráfico de dispersão; ou pela meio da correlação de Pearson. Segue o grafico de dispersão preço da ação em funçao da taxa de juros que mostra uma relação linear crescente. ggplot(dados, aes(x=taxa_de_juros, y=stock_index_price)) + geom_point()+ labs(x=&quot;taxa de juros (%)&quot;, y=&quot;Preço da ação (u.m.)&quot;)+ theme_classic() Agora é o grafico de dispersão do preço da ação em funçao da taxa de desemprego, já mostra uma relação decrescente entre essas variáveis. ggplot(dados, aes(x=taxa_de_desemprego, y=stock_index_price)) + geom_point()+ labs(x=&quot;taxa de desemprego (%)&quot;, y=&quot;Preço da ação (u.m.)&quot;)+ theme_classic() A correlações de Pearson são obtidas pela função cor(). Ao colocar um data frame como argumento da dessa função ela cria a matriz de correlação, que contém todas as correlações possíveis entre as variáveis numéricas. cor1 &lt;- cor(dados$stock_index_price,dados$taxa_de_juros) cor2 &lt;- cor(dados$stock_index_price,dados$taxa_de_desemprego) As medidas correlação entre as variáveis independentes e a variável dependente são próximas de 1 ou de -1, mostrando uma evidência de linearidade nessas relações: A correlação entre preço da ação e taxa de juros é 0.94. A correlação entre preço da ação e taxa de desemprego é -0.92. Ajuste do modelo O Ajuste do modelo será realizado por meio da função lm() e os resultados armazenados no objeto fit1 e a análise desses resutados será com base no relatório gerado pela função summary() fit1 &lt;- lm(stock_index_price ~ taxa_de_juros + taxa_de_desemprego, dados) summary(fit1) ## ## Call: ## lm(formula = stock_index_price ~ taxa_de_juros + taxa_de_desemprego, ## data = dados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -158.205 -41.667 -6.248 57.741 118.810 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1798.4 899.2 2.000 0.05861 . ## taxa_de_juros 345.5 111.4 3.103 0.00539 ** ## taxa_de_desemprego -250.1 117.9 -2.121 0.04601 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 70.56 on 21 degrees of freedom ## Multiple R-squared: 0.8976, Adjusted R-squared: 0.8879 ## F-statistic: 92.07 on 2 and 21 DF, p-value: 4.043e-11 Algumas observações do relatório: O **R-quadrado ajustado reflete o ajuste do modelo, onde um valor mais alto geralmente indica um melhor ajuste O coeficiente intercept é o valor de Y quando as variáveis independentes são zero. O coeficiente taxa de juros é a mudança em Y devido a uma mudança de uma unidade na taxa de juros (todo o resto mantido constante). O coeficiente taxa de desemprego é a mudança em Y devido a uma mudança de uma unidade na taxa de desemprego (todo o resto mantido constante). O erro padrão reflete o nível de precisão dos coeficientes. Pr (&gt; | t |) é o p valor . Um valor de p inferior a 0,05 é considerado estatisticamente significativo. 8.2 Exemplo 2 Análise do tempo de deslocamento, em minutos, de casa à escola, em função da distância, em quilometros, e da quantidade de semafóros. A tabéla abaixo apresenta os dados. Tempo (min) Distância (km) Qtd. Semáforo 15 8 0 20 6 1 20 13 0 40 20 1 50 25 2 23 11 1 10 5 0 55 32 3 35 28 1 30 20 1 Os dados serão armazenados no data frame dados nas variáveis temp - tempo de deslocamento em mínutos. dist - distância, em km, de casa à escola. semf - quantidade de semafóros no trajeto de casa a escola. temp &lt;- c(15, 20, 20, 40, 50, 25, 10, 55, 35, 10) dist &lt;- c(08, 06, 15, 20, 25, 11, 05, 32, 28, 20) semf &lt;- c( 0, 1, 0, 1, 2, 1, 0, 3, 1, 1) dados &lt;- data.frame (temp, dist, semf) Visualização dos dados Em uma regressão linear é necessário que cada variável independente tenha relação linear com a variável dependente. Essa relação pode ser verificado por gráfico de dispersão; ou pela meio da correlação de Pearson. Segue o grafico de dispersão preço da ação em funçao da taxa de juros que mostra uma relação linear crescente. ggplot(dados, aes(x=dist, y=temp)) + geom_point()+ labs(x=&quot;distância (km)&quot;, y=&quot;Tempo (min)&quot;)+ theme_classic() Agora é o grafico de dispersão do preço da ação em funçao da taxa de desemprego, já mostra uma relação decrescente entre essas variáveis. ggplot(dados, aes(x=semf, y=temp)) + geom_point()+ labs(x=&quot;semafóro&quot;, y=&quot;Tempo (min)&quot;)+ theme_classic() A correlações de Pearson são obtidas pela função cor(). Ao colocar um data frame como argumento da dessa função ela cria a matriz de correlação, que contém todas as correlações possíveis entre as variáveis numéricas. cor1 &lt;- cor(dados$temp,dados$dist) cor2 &lt;- cor(dados$temp,dados$semf) As medidas correlação entre as variáveis independentes e a variável dependente são próximas de 1 ou de -1, mostrando uma evidência de linearidade nessas relações: A correlação entre temp e dist é 0.79. A correlação entre temp e semf é 0.84. Ajuste do modelo O Ajuste do modelo será realizado por meio da função lm() e os resultados armazenados no objeto fit1 e a análise desses resutados será com base no relatório gerado pela função summary() fit1 &lt;- lm(temp ~ dist + semf, dados) summary(fit1) ## ## Call: ## lm(formula = temp ~ dist + semf, data = dados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.8757 -1.0643 0.4369 2.7023 10.1243 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.7635 6.2345 1.245 0.2531 ## dist 0.6252 0.4833 1.294 0.2369 ## semf 9.6077 4.8754 1.971 0.0894 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.028 on 7 degrees of freedom ## Multiple R-squared: 0.7583, Adjusted R-squared: 0.6892 ## F-statistic: 10.98 on 2 and 7 DF, p-value: 0.006945 Algumas observações do relatório: O R-quadrado ajustado reflete o ajuste do modelo, onde um valor mais alto geralmente indica um melhor ajuste O coeficiente intercept é o valor de Y quando as variáveis independentes são zero. O coeficiente taxa de juros é a mudança em Y devido a uma mudança de uma unidade na taxa de juros (todo o resto mantido constante). O coeficiente taxa de desemprego é a mudança em Y devido a uma mudança de uma unidade na taxa de desemprego (todo o resto mantido constante). O erro padrão reflete o nível de precisão dos coeficientes. Pr (&gt; | t |) é o p valor . Um valor de p inferior a 0,05 é considerado estatisticamente significativo. 8.3 Exemplo 3 Será estudada a relação do tempo de deslocamento, de casa para a escola, com o a distãncia e número de semáforos. Esse estudo foi já foi realizado Exemplo 2 por meio de matrizes. As variáveis são temp - tempo de deslocamento em mínutos. dist - distância, em km, de casa à escola. semf - quantidade de semafóros no trajeto de casa a escola. dessas a variável temp é a variável dependente e dist e semf são as variáveis independentes. temp &lt;- c(15, 20, 20, 40, 50, 25, 10, 55, 35, 10) dist &lt;- c(08, 06, 15, 20, 25, 11, 05, 32, 28, 20) semf &lt;- c( 0, 1, 0, 1, 2, 1, 0, 3, 1, 1) Y &lt;- matrix (temp,nrow=10,ncol=1) Y ## [,1] ## [1,] 15 ## [2,] 20 ## [3,] 20 ## [4,] 40 ## [5,] 50 ## [6,] 25 ## [7,] 10 ## [8,] 55 ## [9,] 35 ## [10,] 10 X &lt;- matrix (c(c(1,1,1,1,1,1,1,1,1,1), dist,semf), nrow =10, ncol=3) X ## [,1] [,2] [,3] ## [1,] 1 8 0 ## [2,] 1 6 1 ## [3,] 1 15 0 ## [4,] 1 20 1 ## [5,] 1 25 2 ## [6,] 1 11 1 ## [7,] 1 5 0 ## [8,] 1 32 3 ## [9,] 1 28 1 ## [10,] 1 20 1 Cálculo dos betas Os betas são estimados por \\[\\begin{equation*} \\beta = (X^\\prime X)^{-1}X^\\prime Y \\end{equation*}\\] beta &lt;- solve((t(X)%*%X))%*%t(X)%*%Y beta ## [,1] ## [1,] 7.7635256 ## [2,] 0.6252239 ## [3,] 9.6076675 Calcular o erro padrão de \\(\\beta\\) A matriz de variância e covariância de \\(\\beta\\) é estimada por \\[\\begin{equation*} cov\\left(\\hat{\\beta}\\right) = \\left(X^\\prime X\\right)^{-1}\\sigma^2 \\end{equation*}\\] onde \\(X\\) é a matriz de dados e \\(\\sigma^2\\) é estimada por \\[\\begin{equation*} \\hat{\\sigma}^2 = \\frac{Y^\\prime Y - \\hat{\\beta}^\\prime X^\\prime}{n - p -1} \\end{equation*}\\] onde \\(n\\) - número de observações. \\(p\\) - número de variáveis dependentes. \\(X\\) - matriz de dados. \\(Y\\) - dados observados. Essa matriz em sua diagonal apresenta estimativas para as variâncias dos estimadores dos parâmetros \\(\\beta\\), e fora da digonal, estimativas das covarâncias entre as estimativas desses parãmetros. # número de observações n = 10 # número de variáveis independentes p = 2 # cálculo da sigma^2 sigma_square = as.vector((t(Y)%*%Y - t(beta)%*%t(X)%*%Y)/(n-p-1)) sigma_square ## [1] 81.49972 # matriz de variância e covariãncia de beta cov_beta &lt;- solve(t(X)%*%X)*sigma_square cov_beta ## [,1] [,2] [,3] ## [1,] 38.869321 -2.1900677 6.511801 ## [2,] -2.190068 0.2336072 -1.781255 ## [3,] 6.511801 -1.7812550 23.769535 # cálculo do erro padrão de beta var_beta &lt;- diag(cov_beta) std_erro_beta &lt;- sqrt(var_beta) std_erro_beta ## [1] 6.2345265 0.4833293 4.8754010 "]]
